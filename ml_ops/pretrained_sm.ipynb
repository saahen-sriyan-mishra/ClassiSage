{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing depencancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Initialization of constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Change the bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket, region, session\n",
    "bucket_name = 'data-bucket-dkmq0cxe'\n",
    "my_region = boto3.session.Session().region_name\n",
    "sess = boto3.session.Session()\n",
    "print(\"Region is \" + my_region + \" and bucket is \" + bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key (file path within the bucket)\n",
    "file_key = 'final_dataset.csv'\n",
    "\n",
    "# Local path to temporarily save the file\n",
    "local_file_path = 'local_file.csv'\n",
    "\n",
    "# Download the file from S3\n",
    "s3.download_file(bucket_name, file_key, 'local_file.csv')\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('local_file.csv')\n",
    "\n",
    "# Remove the local file after loading it\n",
    "os.remove(local_file_path)\n",
    "\n",
    "# df = df.head(1000) ## TESTING WITH SMALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHECK df.shape \n",
    "##CHECK df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"BlockId\",\"Features\",\"TimeInterval\"] )\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data OutPut Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an output path where the trained model will be saved\n",
    "prefix = 'pretrained-algo'\n",
    "output_path ='s3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Data ML Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows for the 80% split\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "# Extract and shuffle the top 80% of rows\n",
    "_80_df = df.iloc[:split_index].sample(frac=1, random_state=442).reset_index(drop=True)\n",
    "\n",
    "# Extract and shuffle the bottom 20% of rows\n",
    "_20_df = df.iloc[split_index:].sample(frac=1, random_state=352).reset_index(drop=True)\n",
    "\n",
    "# Concatenate the shuffled DataFrames\n",
    "final_df = pd.concat([_80_df, _20_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Split the data into 80% for training and 20% for testing\n",
    "train_data, test_data = np.split(df, [int(0.8 * len(df))])\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = train_data['Label']\n",
    "features = train_data.drop(columns=['Label'])\n",
    "\n",
    "# To Ensure 'Label' Column is the First column in the DataFrame before Saving\n",
    "train_data_final = pd.concat([label_column, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "train_data_final['Label'] = train_data_final['Label'].map({'Success': 1, 'Fail': 0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "train_data_final.to_csv('train.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "# Create an s3_input for the training data\n",
    "s3_input_train = s3_input(s3_data='s3://{}/{}/train/train.csv'.format(bucket_name, prefix), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = test_data['Label']\n",
    "features = test_data.drop(columns=['Label'])\n",
    "\n",
    "test_data_final = pd.concat([label_column, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "test_data_final['Label'] = test_data_final['Label'].map({'Success': 1, 'Fail': 0})  # Map your labels to 1 and 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHECK test_data_final.shape\n",
    "test_data_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "test_data_final.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "# Create an s3_input for the \\testing data\n",
    "s3_input_test = s3_input(s3_data='s3://{}/{}/test/test.csv'.format(bucket_name, prefix), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker SDK/ Inbuild Container "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callling Inbuilt Image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks for the XGBoost image URI and builds an XGBoost container. Specify the repo_version depending on preference.\n",
    "container = get_image_uri(boto3.Session().region_name,\n",
    "                          'xgboost', \n",
    "                          repo_version='1.0-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This constructs the Docker image URI for XGBoost in the specified AWS region and version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",                ## Maximum depth of a tree. Higher means more complex models but risk of overfitting.\n",
    "        \"eta\":\"0.2\",                    ## Learning rate. Lower values make the learning process slower but more precise.\n",
    "        \"gamma\":\"4\",                    ## Minimum loss reduction required to make a further partition on a leaf node. Controls the model’s complexity.\n",
    "        \"min_child_weight\":\"6\",         ## Minimum sum of instance weight (hessian) needed in a child. Higher values prevent overfitting.\n",
    "        \"subsample\":\"0.7\",              ## Fraction of training data used. Reduces overfitting by sampling part of the data. \n",
    "        \"objective\":\"binary:logistic\",  ## Specifies the learning task and corresponding objective. binary:logistic is for binary classification.\n",
    "        \"num_round\":50                  ## Number of boosting rounds, essentially how many times the model is trained.\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container,                  # Points to the XGBoost container we previously set up. This tells SageMaker which algorithm container to use.\n",
    "                                          hyperparameters=hyperparameters,      # Passes the defined hyperparameters to the estimator. These are the settings that guide the training process.\n",
    "                                          role=sagemaker.get_execution_role(),  # Specifies the IAM role that SageMaker assumes during the training job. This role allows access to AWS resources like S3.\n",
    "                                          train_instance_count=1,               # Sets the number of training instances. Here, it’s using a single instance.\n",
    "                                          train_instance_type='ml.m5.large',    # Specifies the type of instance to use for training. ml.m5.2xlarge is a general-purpose instance with a balance of compute, memory, and network resources.\n",
    "                                          train_volume_size=5, # 5GB            # Sets the size of the storage volume attached to the training instance, in GB. Here, it’s 5 GB.\n",
    "                                          output_path=output_path,              # Defines where the model artifacts and output of the training job will be saved in S3.\n",
    "                                          train_use_spot_instances=True,        # Utilizes spot instances for training, which can be significantly cheaper than on-demand instances. Spot instances are spare EC2 capacity offered at a lower price.\n",
    "                                          train_max_run=300,                    # Specifies the maximum runtime for the training job in seconds. Here, it's 300 seconds (5 minutes).\n",
    "                                          train_max_wait=600)                   # Sets the maximum time to wait for the job to complete, including the time waiting for spot instances, in seconds. Here, it's 600 seconds (10 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILE: ~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': s3_input_train,'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment as Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the label column from the test data\n",
    "test_data_features = test_data_final.drop(columns=['Label']).values\n",
    "\n",
    "# Set the content type and serializer\n",
    "xgb_predictor.serializer = CSVSerializer()\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "\n",
    "# Perform prediction\n",
    "predictions = xgb_predictor.predict(test_data_features).decode('utf-8')\n",
    "\n",
    "y_test = test_data_final['Label'].values\n",
    "\n",
    "# Convert the predictions into a array\n",
    "predictions_array = np.fromstring(predictions, sep=',')\n",
    "print(predictions_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting predictions them to binary (0 or 1)\n",
    "threshold = 0.5\n",
    "binary_predictions = (predictions_array >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, binary_predictions)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, binary_predictions)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, binary_predictions)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, binary_predictions)\n",
    "\n",
    "# False Positive Rate (FPR) using the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "false_positive_rate = fp / (fp + tn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.8f}\")\n",
    "print(f\"Precision: {precision:.8f}\")\n",
    "print(f\"Recall: {recall:.8f}\")\n",
    "print(f\"F1 Score: {f1:.8f}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting The EndPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
