{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.35.34)\n",
      "Requirement already satisfied: pandas in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.35.34)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\kiit\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.34->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kiit\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load AWS credentials from terraform.tfvars\n",
    "def load_terraform_vars(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = {}\n",
    "        for line in f:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                key, value = line.replace('\"', '').strip().split('=')\n",
    "                data[key.strip()] = value.strip()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Change the region as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "vars = load_terraform_vars('../terraform.tfvars')\n",
    "access_key = vars['access_key']\n",
    "secret_key = vars['secret_key']\n",
    "region = \"eu-west-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import get_bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to get Terraform output...\n",
      "Raw Terraform Output: {\n",
      "  \"bucket_name\": {\n",
      "    \"sensitive\": false,\n",
      "    \"type\": \"string\",\n",
      "    \"value\": \"data-bucket-axhq3rp8\"\n",
      "  },\n",
      "  \"pretrained_ml_instance_name\": {\n",
      "    \"sensitive\": false,\n",
      "    \"type\": \"string\",\n",
      "    \"value\": \"pretrained-ml-instance\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Parsed Outputs: {\n",
      "    \"bucket_name\": {\n",
      "        \"sensitive\": false,\n",
      "        \"type\": \"string\",\n",
      "        \"value\": \"data-bucket-axhq3rp8\"\n",
      "    },\n",
      "    \"pretrained_ml_instance_name\": {\n",
      "        \"sensitive\": false,\n",
      "        \"type\": \"string\",\n",
      "        \"value\": \"pretrained-ml-instance\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get bucket name\n",
    "bucket_name = get_bucket_name()\n",
    "#print(f\"Bucket Name: {bucket_name}\") ##TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Define the path to CSV files\n",
    "path_eom = \"../data/Event_occurance_matrix\"\n",
    "path_et = \"../data/Event_traces\"\n",
    "\n",
    "# glob to get all CSV files\n",
    "csv_files_eom = glob.glob(f\"{path_eom}/*.csv\")\n",
    "csv_files_et = glob.glob(f\"{path_et}/*.csv\")\n",
    "\n",
    "# List to hold the DFs\n",
    "dfs_eom = []\n",
    "dfs_et = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop and read each .csv file into a DF, then append\n",
    "for file in csv_files_eom:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs_eom.append(df)\n",
    "\n",
    "for file in csv_files_et:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs_et.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all DFs into a single DFs\n",
    "merged_df_EOM = pd.concat(dfs_eom, ignore_index=True)\n",
    "merged_df_ET = pd.concat(dfs_et, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "merged_df_EOM.shape\n",
    "merged_df_ET.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select needed columns\n",
    "filtered_df_EOM = merged_df_EOM[[\"BlockId\", \"Type\"] + [f\"E{i}\" for i in range(1, 30)]]\n",
    "filtered_df_ET = merged_df_ET[[\"BlockId\", \"Features\", \"TimeInterval\", \"Latency\", \"Label\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "filtered_df_EOM.shape  \n",
    "fltered_df_ET.shape  \n",
    "filtered_df_EOM.info()  \n",
    "filtered_df_ET.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DFs on BlockId\n",
    "final_df = pd.merge(filtered_df_ET, filtered_df_EOM, on=\"BlockId\")\n",
    "##CHECK final_df.info()\n",
    "##CHECK final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(columns=\"Type\")\n",
    "##CHECK final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for df_fail and df_success\n",
    "df_fail = pd.DataFrame(columns=final_df.columns)\n",
    "df_success = pd.DataFrame(columns=final_df.columns)\n",
    "\n",
    "# Separate rows based on 'Label'\n",
    "df_fail = pd.concat([df_fail, final_df[final_df['Label'] == 'Fail']], ignore_index=True)\n",
    "df_success = pd.concat([df_success, final_df[final_df['Label'] == 'Success']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for clean DataFrames\n",
    "df_fail.reset_index(drop=True, inplace=True)\n",
    "df_success.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "print(\"FAIL DataFrame:\")\n",
    "df_fail.head(5)\n",
    "df_fail.shape\n",
    "print(\"SUCCESS DataFrame:\")\n",
    "df_success.head(5)\n",
    "df_success.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of 84,165 rows from df_success\n",
    "df_success_sample = df_success.sample(n=84165, random_state=424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df_fail into 80% and 20%\n",
    "split_index_fail = int(len(df_fail) * 0.8)\n",
    "_80_df_fail = df_fail.iloc[:split_index_fail]\n",
    "_20_df_fail = df_fail.iloc[split_index_fail:]\n",
    "\n",
    "# Splitting df_success into 80% and 20%\n",
    "split_index_success = int(len(df_success_sample) * 0.8)\n",
    "_80_df_success = df_success_sample.iloc[:split_index_success]\n",
    "_20_df_success = df_success_sample.iloc[split_index_success:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 80% datasets\n",
    "_80_df = pd.concat([_80_df_fail, _80_df_success], ignore_index=True)\n",
    "# Merging 20% datasets\n",
    "_20_df = pd.concat([_20_df_fail, _20_df_success], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling 80% dataset\n",
    "_80_shuffled_df = _80_df.sample(frac=1, random_state=352).reset_index(drop=True)\n",
    "# Shuffling 20% dataset\n",
    "_20_shuffled_df = _20_df.sample(frac=1, random_state=433).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining shuffled datasets\n",
    "shuffled_df = pd.concat([_80_shuffled_df, _20_shuffled_df], ignore_index=True)\n",
    "\n",
    "##CHECK shuffled_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Success    84165\n",
      "Fail       15835\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## CHECk\n",
    "label_counts = final_df['Label'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Data Into AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the S3 client (with credentials from earlier)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key,\n",
    "    region_name=region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file ../data/final_dataset.csv created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#Local Path\n",
    "local_file_path = '../data/final_dataset.csv'\n",
    "\n",
    "# Save the DF to the CSV file (local machine)\n",
    "final_df.to_csv(local_file_path, index=False)\n",
    "\n",
    "# Debug: Check for successful creation\n",
    "if os.path.exists(local_file_path):\n",
    "    print(f\"Local file {local_file_path} created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create local file {local_file_path}.\")\n",
    "\n",
    "# Define the S3 key (name of the file in the bucket)\n",
    "file_key = \"final_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: data-bucket-axhq3rp8\n",
      "File Key (S3 Filename): final_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print bucket name and file key \n",
    "print(f\"Bucket Name: {bucket_name}\") ## Check\n",
    "print(f\"File Key (S3 Filename): {file_key}\") ## Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try block executing\n",
      "Successfully uploaded final_dataset.csv to data-bucket-axhq3rp8\n",
      "Local file ../data/final_dataset.csv deleted after upload.\n"
     ]
    }
   ],
   "source": [
    "# Try to upload the local CSV file to the S3 bucket\n",
    "try:\n",
    "    print(f\"try block executing\")\n",
    "    s3.upload_file(\n",
    "        Filename=local_file_path, \n",
    "        Bucket=bucket_name,       \n",
    "        Key=file_key               # S3 file key (filename in the bucket)\n",
    "    )\n",
    "    print(f\"Successfully uploaded {file_key} to {bucket_name}\")\n",
    "    \n",
    "    # Delete the local file after uploading to S3\n",
    "    os.remove(local_file_path)\n",
    "    print(f\"Local file {local_file_path} deleted after upload.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to upload file: {e}\")\n",
    "    os.remove(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY RUN CODE BELOW AFTER NOTEBOOK INSTANCE's `pretrained_sm.ipynb` FILE's EXECUTION IS OVER IN SAGEMAKER, BUT BEFORE LAST LINE 2 CODE CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name: data-bucket-axhq3rp8\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def download_directory(bucket_name, local_directory):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name)\n",
    "    \n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                # Get the file key from S3\n",
    "                file_key = obj['Key']\n",
    "                # Local file path where the file will be saved\n",
    "                local_file_path = os.path.join(local_directory, file_key)\n",
    "                \n",
    "                # Create local directories if they don't exist\n",
    "                if not os.path.exists(os.path.dirname(local_file_path)):\n",
    "                    os.makedirs(os.path.dirname(local_file_path))\n",
    "                \n",
    "                # Download the file from S3\n",
    "                try:\n",
    "                    s3.download_file(bucket_name, file_key, local_file_path)\n",
    "                    print(f\"Downloaded {file_key} to {local_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {file_key}: {e}\")\n",
    "\n",
    "# Set the local directory where you want to download the files\n",
    "local_directory = \"../downloaded_bucket_content\"\n",
    "print(f\"Bucket Name: {bucket_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded final_dataset.csv to ../downloaded_bucket_content\\final_dataset.csv\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/claim.smd to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/claim.smd\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/collections/000000000/worker_0_collections.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/collections/000000000/worker_0_collections.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000000/000000000000_worker_0.tfevents to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000000/000000000000_worker_0.tfevents\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000010/000000000010_worker_0.tfevents to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000010/000000000010_worker_0.tfevents\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000020/000000000020_worker_0.tfevents to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000020/000000000020_worker_0.tfevents\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000030/000000000030_worker_0.tfevents to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000030/000000000030_worker_0.tfevents\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000040/000000000040_worker_0.tfevents to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/events/000000000040/000000000040_worker_0.tfevents\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000000_worker_0.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000000_worker_0.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000010_worker_0.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000010_worker_0.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000020_worker_0.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000020_worker_0.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000030_worker_0.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000030_worker_0.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000040_worker_0.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/index/000000000/000000000040_worker_0.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/training_job_end.ts to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/debug-output/training_job_end.ts\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/output/model.tar.gz to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/output/model.tar.gz\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/framework/training_job_end.ts to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/framework/training_job_end.ts\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496400.algo-1.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496400.algo-1.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496460.algo-1.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496460.algo-1.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496520.algo-1.json to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/incremental/2024102107/1729496520.algo-1.json\n",
      "Downloaded pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/training_job_end.ts to ../downloaded_bucket_content\\pretrained-algo/output/sagemaker-xgboost-2024-10-21-07-40-14-736/profiler-output/system/training_job_end.ts\n",
      "Downloaded pretrained-algo/test/test.csv to ../downloaded_bucket_content\\pretrained-algo/test/test.csv\n",
      "Downloaded pretrained-algo/train/train.csv to ../downloaded_bucket_content\\pretrained-algo/train/train.csv\n",
      "Downloaded pretrained_sm.ipynb to ../downloaded_bucket_content\\pretrained_sm.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function to download the entire bucket\n",
    "try:\n",
    "    download_directory(bucket_name, local_directory)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
